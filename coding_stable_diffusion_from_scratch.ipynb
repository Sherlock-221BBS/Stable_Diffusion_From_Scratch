{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPb+AIWxPvef67sKntbY/gP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sherlock-221BBS/Stable_Diffusion_From_Scratch/blob/main/coding_stable_diffusion_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "\n"
      ],
      "metadata": {
        "id": "nhy4amAXwDxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention"
      ],
      "metadata": {
        "id": "QJSgyUrbwOsm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RA049l4H8M1f"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, n_heads, d_embed, in_proj_bias = True, out_proj_bias = True):\n",
        "    super().__init__()\n",
        "    self.in_proj = nn.Linear(in_features = d_embed, out_features = d_embed * 3, bias = in_proj_bias)\n",
        "    self.out_proj = nn.Linear(in_features = d_embed, out_features = d_embed, bias = out_proj_bias)\n",
        "    self.n_heads = n_heads\n",
        "    self.d_head = d_embed // n_heads\n",
        "\n",
        "  def forward(self, x, causal_mask = False):\n",
        "    input_shape = x.shape\n",
        "    batch, seq_len, d_embed = input_shape\n",
        "\n",
        "    q, k, v = self.in_proj(x).chunk(3, dim = -1)\n",
        "    interim_shape = (batch, seq_len, self.n_heads, self.d_head)\n",
        "\n",
        "    q = q.reshape(interim_shape).transpose(1, 2)\n",
        "    k = k.reshape(interim_shape).transpose(1, 2)\n",
        "    v = v.reshape(interim_shape).transpose(1, 2)\n",
        "\n",
        "    weights = q @ k.transpose(-1, -2)\n",
        "\n",
        "    if causal_mask:\n",
        "      mask = torch.ones_like(weights, dtype = torch.bool).triu(1)\n",
        "      weights.mask_fill(mask, -torch.inf)\n",
        "    weights /= math.sqrt(self.d_head)\n",
        "    weights = F.softmax(weights, dim = -1)\n",
        "\n",
        "    outputs = weights @ v\n",
        "    outputs = outputs.transpose(1, 2)\n",
        "    outputs = outputs.reshape(input_shape)\n",
        "    outputs = self.out_proj(outputs)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VAE Helper Classes"
      ],
      "metadata": {
        "id": "mVW_bqtvwYL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE_AttentionBlock(nn.Module):\n",
        "  def __init__(self, channels):\n",
        "    super().__init__()\n",
        "    self.groupnorm = nn.GroupNorm(32, channels)\n",
        "    self.attention = SelfAttention(1, channels)\n",
        "\n",
        "  def forward(self, x):\n",
        "    residue = x\n",
        "    x = self.groupnorm(x)\n",
        "    n, c, h, w = x.shape\n",
        "    x = x.view((n, c, h * w))\n",
        "    x = x.transpose(-1, -2)\n",
        "    x = self.attention(x)\n",
        "    x = x.view((n, c, h, w))\n",
        "    return x + residue\n",
        "\n",
        "class VAE_ResidualBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super().__init__()\n",
        "    self.groupnorm_1 = nn.GroupNorm(32, in_channels)\n",
        "    self.conv_1 = nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1)\n",
        "    self.groupnorm_2 = nn.GroupNorm(32, out_channels)\n",
        "    self.conv_2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, padding = 1)\n",
        "    if in_channels == out_channels:\n",
        "      self.residual_layer = nn.Identity()\n",
        "    else:\n",
        "      self.residual_layer = nn.Conv2d(in_channels, out_channels, kernel_size = 3,padding = 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    residue = x\n",
        "    x = self.groupnorm_1(x)\n",
        "    x = F.silu(x)\n",
        "    x = self.conv_1(x)\n",
        "    x = self.groupnorm_2(x)\n",
        "    x = F.silu(x)\n",
        "    x = self.conv_2(x)\n",
        "    print(\"successfully passed\")\n",
        "    return x + self.residual_layer(residue)\n"
      ],
      "metadata": {
        "id": "xAFtbFRkwpop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder of VAE"
      ],
      "metadata": {
        "id": "d4d_4DtNwrbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "class VAE_Encoder(nn.Sequential):\n",
        "  def __init__(self):\n",
        "    super().__init__(\n",
        "        nn.Conv2d(in_channels = 3,\n",
        "                  out_channels = 128,\n",
        "                  kernel_size = 3,\n",
        "                  padding = 1),\n",
        "        VAE_ResidualBlock(128, 128),\n",
        "        VAE_ResidualBlock(128, 128),\n",
        "        nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 3, stride = 2, padding = 0),\n",
        "        VAE_ResidualBlock(128, 256),\n",
        "        VAE_ResidualBlock(256, 256),\n",
        "        nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, stride = 2, padding = 0),\n",
        "        VAE_ResidualBlock(256, 512),\n",
        "        VAE_ResidualBlock(512, 512),\n",
        "        nn.Conv2d(in_channels = 512, out_channels =512, kernel_size = 3, stride = 2, padding =0),\n",
        "        VAE_ResidualBlock(512, 512),\n",
        "        VAE_ResidualBlock(512, 512),\n",
        "        VAE_ResidualBlock(512, 512),\n",
        "        VAE_AttentionBlock(512),\n",
        "        VAE_ResidualBlock(512, 512),\n",
        "        nn.GroupNorm(32, 512),\n",
        "        nn.SiLU(),\n",
        "        nn.Conv2d(in_channels = 512, out_channels = 8, kernel_size = 3, padding =1)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x, noise):\n",
        "    for module in self:\n",
        "      if getattr(module, \"stride\", None) == (2, 2):\n",
        "        x = F.pad(x, (0, 1, 0, 1))\n",
        "\n",
        "      x = module(x)\n",
        "\n",
        "    mean, log_variance = x.chunk(2, dim = 1)\n",
        "    log_variance = torch.clamp(log_variance, min = -30, max = 20)\n",
        "    variance = log_variance.exp()\n",
        "    std_dev = variance.sqrt()\n",
        "    x = mean + noise * std_dev\n",
        "    return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FWzMn_adw7d7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vae_encoder = VAE_Encoder()\n",
        "input = torch.randn(1, 3, 512, 512)\n",
        "noise = torch.randn(1, 4, 64, 64)\n",
        "encoder_outputs = vae_encoder(input, noise)\n",
        "encoder_outputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f66qo-33i819",
        "outputId": "b5276436-67ae-495e-8a76-17d4bcb6351b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "successfully passed\n",
            "successfully passed\n",
            "successfully passed\n",
            "successfully passed\n",
            "successfully passed\n",
            "successfully passed\n",
            "successfully passed\n",
            "successfully passed\n",
            "successfully passed\n",
            "successfully passed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 64, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder of VAE"
      ],
      "metadata": {
        "id": "XjWquOgEahRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE_Decoder(nn.Sequential):\n",
        "  def __init__(self):\n",
        "    super().__init__(\n",
        "        nn.Conv2d(4, 4, kernel_size = 1, padding = 0),\n",
        "        nn.Conv2d(4, 512, kernel_size = 3, padding = 1),\n",
        "        VAE_ResidualBlock(512, 512),\n",
        "        VAE_AttentionBlock(512),\n",
        "        VAE_ResidualBlock(512, 512),\n",
        "        VAE_ResidualBlock(512, 512),\n",
        "        VAE_ResidualBlock(512, 512),\n",
        "        VAE_ResidualBlock(512, 512),\n",
        "        nn.Upsample(scale_factor = 2),\n",
        "        nn.Conv2d(512, 512, kernel_size = 3, padding = 1),\n",
        "        VAE_ResidualBlock(512, 512),\n",
        "        VAE_ResidualBlock(512, 512),\n",
        "        VAE_ResidualBlock(512, 512),\n",
        "        nn.Upsample(scale_factor = 2),\n",
        "        nn.Conv2d(512, 512, kernel_size = 3, padding =1),\n",
        "        VAE_ResidualBlock(512, 256),\n",
        "        VAE_ResidualBlock(256, 256),\n",
        "        VAE_ResidualBlock(256, 256),\n",
        "        nn.Upsample(scale_factor = 2),\n",
        "        nn.Conv2d(256, 256, kernel_size = 3, padding = 1),\n",
        "        VAE_ResidualBlock(256, 128),\n",
        "        VAE_ResidualBlock(128, 128),\n",
        "        VAE_ResidualBlock(128, 128),\n",
        "        nn.GroupNorm(32, 128),\n",
        "        nn.SiLU(),\n",
        "        nn.Conv2d(128, 3, kernel_size = 3, padding = 1)\n",
        "\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x /= 0.18125\n",
        "    for module in self:\n",
        "      x = module(x)\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "FfvKOvwCjSXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vae_decoder = VAE_Decoder()\n",
        "decoder_output = vae_decoder(encoder_outputs)\n",
        "decoder_output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trkzcP3N8EU7",
        "outputId": "e8fcf9cb-c3bd-4016-9cc9-3eee9f2a010b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "successfully passed\n",
            "successfully passed\n",
            "successfully passed\n",
            "successfully passed\n",
            "successfully passed\n",
            "successfully passed\n",
            "successfully passed\n",
            "successfully passed\n",
            "successfully passed\n",
            "successfully passed\n",
            "successfully passed\n",
            "successfully passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPEmbedding(nn.Module):\n",
        "  def __init__(self, n_vocab, n_embd, n_tokens):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(n_vocab, n_embd)\n",
        "    self.positional_embedding = nn.Parameter(torch.randn(n_tokens, n_embd))\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embedding(x)\n",
        "    x+= self.positional_embedding\n",
        "    return x\n",
        "\n",
        "\n",
        "class CLIPLayer(nn.Module):\n",
        "  def __init__(self, n_head, n_embd):\n",
        "    super().__init__()\n",
        "    self.layernorm_1 = nn.LayerNorm(n_embd)\n",
        "    self.attention = SelfAttention(n_head, n_embd)\n",
        "\n",
        "    self.layernorm_2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    self.linear_1 = nn.Linear(n_embd, 4 * n_embd)\n",
        "    self.linear_2 = nn.Linear(4 * n_embd, n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    residue = x\n",
        "    x = self.layernorm_1(x)\n",
        "    print(\"first normalization layer passed\")\n",
        "    x = self.attention(x)\n",
        "    print(\"first attention layer passed\")\n",
        "    x+= residue\n",
        "\n",
        "    residue = x\n",
        "    x = self.layernorm_2(x)\n",
        "    x = self.linear_1(x)\n",
        "    x = x * torch.sigmoid(1.702 * x)\n",
        "    x = self.linear_2(x)\n",
        "\n",
        "    x+= residue\n",
        "    return x\n",
        "\n",
        "class CLIP(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.embedding = CLIPEmbedding(49708, 768, 77)\n",
        "    self.layers = nn.ModuleList([\n",
        "        CLIPLayer(12, 768) for i in range(12)\n",
        "    ])\n",
        "    self.layernorm = nn.LayerNorm(768)\n",
        "\n",
        "\n",
        "  def forward(self, tokens):\n",
        "    tokens = tokens.type(torch.long)\n",
        "    state = self.embedding(tokens)\n",
        "    print(\"embedding layer passed\")\n",
        "\n",
        "    for layer in self.layers:\n",
        "      state = layer(state)\n",
        "\n",
        "    output = self.layernorm(state)\n",
        "\n",
        "    return output\n",
        "\n"
      ],
      "metadata": {
        "id": "2JeVGftR8YKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = torch.randint(1, 49708, (1, 77))\n",
        "clip = CLIP()\n",
        "clip_output = clip(tokens)\n",
        "clip_output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "BanuC_Nljgbr",
        "outputId": "fba7fcdf-4d7c-4210-b438-8fce9e420dd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embedding layer passed\n",
            "first normalization layer passed\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-3f42e284a07e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m49708\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m77\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclip_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mclip_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-3e0daf1543b5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m       \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-3e0daf1543b5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayernorm_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"first normalization layer passed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"first attention layer passed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m+=\u001b[0m \u001b[0mresidue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-980f08adc419>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, causal_mask)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0minterim_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterim_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterim_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterim_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 77, 12, 768]' is invalid for input of size 59136"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ieq-TbXXj5kt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}